{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acarranz/repos/infra-forecast_trunk/build/infra-forecast_2.11/environments/development-venv/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import prestodb\n",
    "from getpass import getpass\n",
    "from linkedin.jiraclient import JIRA\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.file_utils import is_tf_available, is_torch_available\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class containing methods that load u_meeseeks tables via presto\n",
    "\"\"\"\n",
    "class PrestoConnection(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user = input(\"LDAP user name:\")\n",
    "        self.password = getpass(\"LDAP password + 2FA:\")\n",
    "        self.conn = prestodb.dbapi.connect(\n",
    "            host = 'presto-obfuscated.grid.linkedin.com',\n",
    "            port = 8443,\n",
    "            user = self.user,\n",
    "            catalog = 'hive',\n",
    "            schema = 'default',\n",
    "            http_scheme = 'https',\n",
    "            auth = prestodb.auth.BasicAuthentication(self.user, self.password),\n",
    "        )\n",
    "        \n",
    "    def get_incident_checks(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.incident_checks\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        incident_checks = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return incident_checks\n",
    "    \n",
    "    def get_incident_group_matches(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.incident_group_matches\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        incident_group_matches = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return incident_group_matches\n",
    "    \n",
    "    def get_incident_groups(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.incident_groups\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        incident_groups = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return incident_groups\n",
    "    \n",
    "    def get_incidents(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.incidents\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        incidents = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return incidents\n",
    "    \n",
    "    def get_iris_metadata(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.iris_metadata\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        iris_metadata = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return iris_metadata\n",
    "    \n",
    "    def get_iris_steps(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.iris_steps\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        iris_steps = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return iris_steps\n",
    "    \n",
    "    def get_jira_group_matches(self, start_date, end_date):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM u_meeseeks.jira_group_matches\n",
    "            WHERE\n",
    "                datepartition BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        jira_group_matches = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        return jira_group_matches\n",
    "    \n",
    "    def get_all_tables(self, start_date, end_date):\n",
    "        incident_checks = self.get_incident_checks(start_date, end_date)\n",
    "        incident_group_matches = self.get_incident_group_matches(start_date, end_date)\n",
    "        incident_groups = self.get_incident_groups(start_date, end_date)\n",
    "        incidents = self.get_incidents(start_date, end_date)\n",
    "        iris_metadata = self.get_iris_metadata(start_date, end_date)\n",
    "        iris_steps = self.get_iris_steps(start_date, end_date)\n",
    "        jira_group_matches = self.get_jira_group_matches(start_date, end_date)\n",
    "        \n",
    "        dfs = {'incident_checks':incident_checks,\n",
    "              'incident_group_matches':incident_group_matches,\n",
    "              'incident_groups':incident_groups,\n",
    "              'incidents':incidents,\n",
    "              'iris_metadata':iris_metadata,\n",
    "              'iris_steps':iris_steps,\n",
    "              'jira_group_matches':jira_group_matches}\n",
    "        \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class containing methods that load Jira tickets data\n",
    "\"\"\"\n",
    "class JiraConnection(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user = input(\"LDAP user name:\")\n",
    "        self.password = getpass(\"LDAP password:\")\n",
    "        self.jira_client = JIRA(user=self.user, password=self.password)\n",
    "\n",
    "    def _extract_jira_values(self, jira_entries):\n",
    "        return_list = []\n",
    "        if jira_entries:\n",
    "            for value in jira_entries:\n",
    "                return_list.append(value[\"value\"].strip())\n",
    "        return return_list\n",
    "\n",
    "    def _extract_jira_comments(self, jira_comments):\n",
    "        return_list = []\n",
    "        if jira_comments:\n",
    "            for jira_comment in jira_comments:\n",
    "                author = jira_comment[\"author\"][\"name\"]\n",
    "                update_author = jira_comment.get(\"updateAuthor\", {}).get(\"name\", \"\")\n",
    "                created = jira_comment[\"created\"]\n",
    "                updated = jira_comment.get(\"updated\", None)\n",
    "                comment = jira_comment[\"body\"]\n",
    "                return_list.append({\"author\":author,\n",
    "                                    \"update_author\":update_author,\n",
    "                                    \"created\":created,\n",
    "                                    \"updated\":updated,\n",
    "                                    \"comment\":comment})\n",
    "        return return_list\n",
    "\n",
    "    def _jira_issue_to_dict(self, jira_issue):\n",
    "        \"\"\"\n",
    "        Takes a given jira issue and converts it to a dictionary\n",
    "        :param jira_issue: jira issue returned by a jira client lookup\n",
    "        :return: dictionary containing information from Jira\n",
    "        \"\"\"\n",
    "        jira_ticket_id = jira_issue.key\n",
    "        issue_fields = jira_issue.raw[\"fields\"]\n",
    "\n",
    "        issue_type = issue_fields.get(\"issuetype\", {}).get(\"name\", \"\")\n",
    "        issue_labels = issue_fields.get(\"labels\", [])\n",
    "\n",
    "        severity = issue_fields.get(\"Severity\", {}).get(\"value\", \"\")\n",
    "        user_impact_data = issue_fields.get(\"User_Impact\", \"\")\n",
    "        user_impact = user_impact_data if user_impact_data else \"\"\n",
    "        impact = issue_fields.get(\"Impact\", {}).get(\"value\", \"\")\n",
    "\n",
    "        products_impacted = self._extract_jira_values(issue_fields.get(\"customfield_14201\", []))\n",
    "        devices_impacted = self._extract_jira_values(issue_fields.get(\"customfield_14202\", []))\n",
    "        fabrics_impacted = self._extract_jira_values(issue_fields.get(\"customfield_14203\", []))\n",
    "        impacted_teams = self._extract_jira_values(issue_fields.get(\"customfield_10356\", []))\n",
    "        responsible_teams = self._extract_jira_values(issue_fields.get(\"customfield_10570\", []))\n",
    "        responsible_services = self._extract_jira_values(issue_fields.get(\"customfield_17001\", []))\n",
    "        issue_detected_via = self._extract_jira_values(issue_fields.get(\"Issue_Detected_Via\", []))\n",
    "\n",
    "        summary = issue_fields.get(\"summary\", \"\")\n",
    "        description = issue_fields.get(\"description\", \"\")\n",
    "        domain_data = issue_fields.get(\"Domain\", {})\n",
    "        domain = domain_data.get(\"value\", \"\") if domain_data else \"\"\n",
    "\n",
    "        issue_start_time = issue_fields.get(\"Start_Time\", None)\n",
    "        issue_detection_time = issue_fields.get(\"Issue_Detection_Time\", None)\n",
    "        issue_mitigation_time = issue_fields.get(\"Issue_Mitigation_Time\", None)\n",
    "        issue_resolution_time = issue_fields.get(\"Issue_Resolution_Time\", None)\n",
    "\n",
    "        jira_comments = self._extract_jira_comments(issue_fields.get(\"comment\", {}).get(\"comments\", []))\n",
    "\n",
    "        root_cause = issue_fields.get(\"Root_Cause\", \"\")\n",
    "        # its possible for this field to exist but to be None, hence the extra guarding\n",
    "        root_cause_category_data = issue_fields.get(\"Root_Cause_Category\", {})\n",
    "        root_cause_category = root_cause_category_data.get(\"value\", \"\") if root_cause_category_data else \"\"\n",
    "\n",
    "        jira_root_cause_mapping = {\"Code Change / Full deployment\" : \"Deploy\",\n",
    "                                   \"Configuration\" : \"Deploy - Config\",\n",
    "                                   \"Capacity\" : \"Capacity\",\n",
    "                                   \"Infrastructure Failure\" : \"Infrastructure\",\n",
    "                                   \"Hardware Failure\" : \"Hardware\",\n",
    "                                   \"Single Node Failure\" : \"Outlier Bad Hosts\",\n",
    "                                   \"Code Change\" : \"Deploy\",\n",
    "                                   \"CM\" : \"CM\",\n",
    "                                   \"Code Change / Canary\" : \"Deploy\",\n",
    "                                   \"LIX / Feature Ramp\" : \"LIX\",\n",
    "                                   \"Bug\" : \"Deploy\"}\n",
    "\n",
    "        jira_label = jira_root_cause_mapping.get(root_cause_category, \"\")\n",
    "            \n",
    "        return {\"jira_ticket_id\": jira_ticket_id,\n",
    "                \"jira_label\": jira_label,\n",
    "                \"root_cause_category\": root_cause_category,\n",
    "                \"root_cause\": root_cause,\n",
    "                \"issue_type\": issue_type,\n",
    "                \"issue_labels\" : issue_labels,\n",
    "                \"severity\": severity,\n",
    "                \"user_impact\": user_impact,\n",
    "                \"impact\": impact,\n",
    "                \"products_impacted\": products_impacted,\n",
    "                \"devices_impacted\": devices_impacted,\n",
    "                \"fabrics_impacted\": fabrics_impacted,\n",
    "                \"impacted_teams\": impacted_teams,\n",
    "                \"responsible_teams\": responsible_teams,\n",
    "                \"responsible_services\": responsible_services,\n",
    "                \"issue_detected_via\": issue_detected_via,\n",
    "                \"summary\": summary,\n",
    "                \"description\": description,\n",
    "                \"domain\": domain,\n",
    "                \"jira_comments\" : jira_comments}\n",
    "\n",
    "    def get_jira_tickets(self, jira_group_matches):\n",
    "        tickets = []\n",
    "        for jira_ticket_id in jira_group_matches[\"jira_ticket_id\"].unique():\n",
    "            try:\n",
    "                jira_issue = self.jira_client.issue(jira_ticket_id)\n",
    "                jira_dict = self._jira_issue_to_dict(jira_issue)\n",
    "                tickets.append(jira_dict)\n",
    "            except:\n",
    "                print(\"Jira ticket\", jira_ticket_id, \"not found.\")  \n",
    "\n",
    "        return tickets\n",
    "    \n",
    "    def get_jira_text(self, jira_group_matches):\n",
    "        tickets = self.get_jira_tickets(jira_group_matches)\n",
    "        \n",
    "        jira_comments = []\n",
    "        for ticket in tickets:\n",
    "            text = \"\"        \n",
    "            if ticket[\"root_cause\"]:\n",
    "                text += ticket[\"root_cause\"] + \" \"\n",
    "            if ticket[\"summary\"]:\n",
    "                text += ticket[\"summary\"] + \" \"\n",
    "            if ticket[\"description\"]:\n",
    "                text += ticket[\"description\"] + \" \"\n",
    "            for comment in ticket[\"jira_comments\"]:\n",
    "                text += comment[\"comment\"] + \" \"\n",
    "\n",
    "            data = {\"jira_ticket_id\":ticket[\"jira_ticket_id\"], \"jira_label\":ticket[\"jira_label\"], \"jira_text\":text}\n",
    "            jira_comments.append(data)\n",
    "            \n",
    "        return pd.DataFrame(jira_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Presto\n",
    "presto = PrestoConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Jira API\n",
    "jira = JiraConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Meeseeks data\n",
    "\"\"\"\n",
    "\n",
    "# Define data range\n",
    "start_date = \"2021-04-01-00\"\n",
    "end_date = datetime.date.today().strftime(\"%Y-%m-%d-00\")\n",
    "\n",
    "# Load incident to incident group dataframe\n",
    "incident_group_matches = presto.get_incident_group_matches(start_date, end_date)\n",
    "\n",
    "# Load incident group to jira ticket dataframe\n",
    "jira_group_matches = presto.get_jira_group_matches(start_date, end_date)\n",
    "\n",
    "# Load incident groups metadata and comment information dataframe\n",
    "incident_groups = presto.get_incident_groups(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Jira-Meeseeks tickets data\n",
    "\"\"\"\n",
    "\n",
    "# If data previously saved, skip extraction\n",
    "reload_data = False\n",
    "if reload_data:\n",
    "    # Feed jira_group_matches dataframe to load jira tickets associated to incident groups\n",
    "    jira_meeseeks_df = jira.get_jira_text(jira_group_matches)\n",
    "    \n",
    "    # Save data\n",
    "    jira_meeseeks_df.to_csv(\"./data/jira_meeseeks.csv\")\n",
    "    \n",
    "# Load data\n",
    "jira_meeseeks_df = pd.read_csv(\"./data/jira_meeseeks.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "jira_meeseeks_df[\"jira_label\"] = jira_meeseeks_df[\"jira_label\"].mask(jira_meeseeks_df[\"jira_label\"].isna(), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attach labels to Meeseeks data\n",
    "\n",
    "Note: This is specific to the CSV file containing labeled incidents. Adjust accordingly.\n",
    "\"\"\"\n",
    "\n",
    "# Standardized label mapping\n",
    "manual_root_cause_mapping = {\"Noise\" : \"Noise\",\n",
    "                             \"Noise - auto recover\" : \"Noise\",\n",
    "                             \"Downstream\" : \"Downstream\",\n",
    "                             \"Downstream - Different\" : \"Downstream\",\n",
    "                             \"Downstream - different\" : \"Downstream\",\n",
    "                             \"Downstream - Capacity\" : \"Downstream\", \n",
    "                             \"Downstream - Dyno\" : \"Downstream\",\n",
    "                             \"Deploy\" : \"Deploy\",\n",
    "                             \"Deploy - Canary\" : \"Deploy\",\n",
    "                             \"Deploy - Config\" : \"Deploy\",\n",
    "                             \"Deploy - restart\" : \"Deploy\",\n",
    "                             \"Outlier Bad Hosts\" : \"Outlier Bad Hosts\",\n",
    "                             \"Outlier Host\" : \"Outlier Bad Hosts\",\n",
    "                             \"Outlier Host - GC\" : \"Outlier Bad Hosts\",\n",
    "                             \"GC\" : \"Garbage Collection\",\n",
    "                             \"Capacity\" : \"Capacity\",\n",
    "                             \"Capacity - Quota\" : \"Capacity\",\n",
    "                             \"Database\" : \"Database\",\n",
    "                             \"Database - MySQL\" : \"Database\",\n",
    "                             \"Upstream\" : \"Upstream\",\n",
    "                             \"CM\" : \"CM\",\n",
    "                             \"CM - Planned Maintenance\" : \"CM\",\n",
    "                             \"CM - Traffic Shift\" : \"CM\",\n",
    "                             \"CM - Load Test\" : \"CM\",\n",
    "                             \"Couchbase\" : \"Database\",\n",
    "                             \"Espresso\" : \"Database\"}\n",
    "\n",
    "# Load labeled incidents data\n",
    "manual_label_dir = \"./data/panel_title_data_2021-07-22.csv\"\n",
    "labels_df = pd.read_csv(manual_label_dir).drop(\"id\", axis=1)[[\"incident_id\",\"label\"]]\n",
    "labels_df[\"label\"] = labels_df[\"label\"].astype(str).apply(str.strip)\n",
    "\n",
    "# Merge labeled incidents data with incident_group cross-reference table\n",
    "labels_df = pd.merge(incident_group_matches[[\"incident_group_id\", \"incident_id\"]], labels_df, how=\"right\", on=\"incident_id\")\n",
    "labels_df = labels_df[labels_df[\"incident_group_id\"].notna()]\n",
    "labels_df.incident_group_id = labels_df.incident_group_id.astype('int64')\n",
    "\n",
    "# Group incidents into incident group and extract first label in group\n",
    "# This assumes all incidents in an incident group have the same label\n",
    "labels_df = labels_df[[\"incident_group_id\",\"label\"]].groupby('incident_group_id').first().reset_index()\n",
    "labels_df = labels_df[labels_df[\"label\"].notna()]\n",
    "\n",
    "# Map labels to standardized labels\n",
    "labels_df[\"label\"] = labels_df[\"label\"].apply(lambda x: x.strip())\n",
    "labels_df[\"label\"] = labels_df[\"label\"].apply(lambda x: x if x==\"\" else manual_root_cause_mapping.get(x.strip(), \"Other\"))\n",
    "\n",
    "# Merge labeled incident groups to incident groups data\n",
    "incidents_df = pd.merge(labels_df, incident_groups, how=\"right\", on=\"incident_group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add additional incident group labels\n",
    "\n",
    "Note: This is specific to the labeled incident groups. Adjust accordingly.\n",
    "\"\"\"\n",
    "\n",
    "# Load manually labeled incident groups data\n",
    "manual_label_dir2 = \"./data/labeled_groups.csv\"\n",
    "labels2_df = pd.read_csv(manual_label_dir2).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# Pre-process labels\n",
    "labels2_df = labels2_df[~labels2_df[\"actual\"].isin([\"?\",\"I don't know\"])]\n",
    "labels2_df = labels2_df[labels2_df[\"actual\"].notna()]\n",
    "labels2_df[\"actual\"] = labels2_df[\"actual\"].astype(str).apply(lambda x: x.split()[0])\n",
    "labels2_df = labels2_df[[\"incident_group_id\", \"actual\"]]\n",
    "\n",
    "# Merge with labeled incident groups data\n",
    "incidents_df = pd.merge(incidents_df, labels2_df, how=\"left\", on=\"incident_group_id\")\n",
    "\n",
    "# Include additional labels in label column where labels did not exist\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].mask((incidents_df[\"label\"]==\"\") | (incidents_df[\"label\"].isna()), incidents_df[\"actual\"])\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].mask(incidents_df[\"label\"].isna(), \"\")\n",
    "incidents_df = incidents_df.drop(\"actual\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define final incident groups data format. \n",
    "\"\"\"\n",
    "\n",
    "# Define text field\n",
    "incidents_df[\"text\"] = incidents_df[\"comment\"]\n",
    "\n",
    "# Choose final data fields\n",
    "incidents_df = incidents_df[[\"incident_group_id\", \"label\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge incident groups with additional Jira-Meeseeks text data\n",
    "\"\"\"\n",
    "\n",
    "# Merge jira-meeseeks tickets data with jira_group_matches cross-reference table\n",
    "jira_text_df = pd.merge(jira_group_matches[[\"incident_group_id\",\"jira_ticket_id\"]], jira_meeseeks_df, how=\"left\", on=\"jira_ticket_id\")\n",
    "\n",
    "# Merge incident groups data with cross-referenced jira tickets data\n",
    "incidents_df = pd.merge(incidents_df, jira_text_df, how=\"left\", on=\"incident_group_id\")\n",
    "\n",
    "# Pre-process jira ticket text and append to text field\n",
    "incidents_df[\"jira_text\"] = incidents_df[\"jira_text\"].mask(incidents_df[\"jira_text\"].isna(), \"\").astype(str)\n",
    "incidents_df[\"text\"] = incidents_df[[\"text\", \"jira_text\"]].agg(' '.join, axis=1)\n",
    "\n",
    "# Pre-process labels\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].mask((incidents_df[\"label\"]==\"\") | (incidents_df[\"label\"].isna()), incidents_df[\"jira_label\"])\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].mask(incidents_df[\"label\"].isna(), \"\")\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].apply(lambda x: x.strip())\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].apply(lambda x: x if x==\"\" else manual_root_cause_mapping.get(x.strip(), \"Other\"))\n",
    "\n",
    "# Choose final data fields\n",
    "incidents_df = incidents_df[[\"incident_group_id\", \"label\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save incident groups data\n",
    "incidents_df.to_csv(\"./data/incidents.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text preprocessing methods\n",
    "\"\"\"\n",
    "from gensim.parsing.preprocessing import (strip_punctuation, strip_multiple_whitespaces,\n",
    "                                          strip_numeric, remove_stopwords, strip_short, stem_text)\n",
    "\n",
    "def strip_links(s):\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    l = []\n",
    "    for word in s.split():\n",
    "        if '.com' not in word:\n",
    "            l.append(word)\n",
    "    return ' '.join(l)\n",
    "\n",
    "def strip_control_characters(s):\n",
    "    s_new = \"\"\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] == \"C\":\n",
    "            s_new += \" \"\n",
    "        else:\n",
    "            s_new += ch\n",
    "            \n",
    "    s_new = s_new.replace(u'\\xa0', u' ')\n",
    "    return s_new\n",
    "\n",
    "def preprocess_text(s):\n",
    "    s = strip_control_characters(s)\n",
    "    s = strip_links(s)\n",
    "    s = strip_punctuation(s)\n",
    "    s = strip_multiple_whitespaces(s)\n",
    "    s = strip_numeric(s)\n",
    "    s = strip_short(s, minsize=3)\n",
    "    s = remove_stopwords(s)\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load incidents data\n",
    "\"\"\"\n",
    "\n",
    "# Read pre-saved data incidents\n",
    "incidents_df = pd.read_csv(\"./data/incidents.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].mask(incidents_df[\"label\"].isna(), \"\")\n",
    "incidents_df[\"text\"] = incidents_df[\"text\"].astype(str)\n",
    "\n",
    "# Preprocess text\n",
    "incidents_df[\"text\"] = incidents_df[\"text\"].apply(lambda s: preprocess_text(s))\n",
    "\n",
    "# Only keep incident groups with non-empty comment\n",
    "incidents_df = incidents_df[incidents_df[\"text\"] != \"\"].copy()\n",
    "\n",
    "# Restrict to a select list of labels. Cluster other labels into \"Other\" label\n",
    "keep_labels = [\"Noise\", \"Downstream\", \"Deploy\", \"Outlier Bad Hosts\", \"\"]\n",
    "incidents_df[\"label\"] = incidents_df[\"label\"].apply(lambda x: x if x in keep_labels else \"Other\")\n",
    "\n",
    "# Define labels list and label index mappings\n",
    "labels_list = [\"Noise\", \"Downstream\", \"Deploy\", \"Outlier Bad Hosts\", \"Other\"]\n",
    "label_map = {label:idx for idx, label in enumerate(labels_list)}\n",
    "inv_label_map = {v:k for k, v in label_map.items()}\n",
    "\n",
    "# Create label index field. Define \"-1\" for incident groups without label\n",
    "incidents_df[\"target\"] = incidents_df[\"label\"].apply(lambda x: label_map[x] if x in label_map else -1)\n",
    "\n",
    "# Choose final data fields\n",
    "incidents_df = incidents_df[[\"incident_group_id\",\"label\",\"target\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acarranz/repos/infra-forecast_trunk/build/infra-forecast_2.11/environments/development-venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3267: DtypeWarning: Columns (19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load additional Jira comments data\n",
    "\"\"\"\n",
    "\n",
    "def convert_to_str(x):\n",
    "    if not x:\n",
    "        return ''\n",
    "    try:\n",
    "        return str(x)   \n",
    "    except:        \n",
    "        return ''\n",
    "    \n",
    "# Load data containing unlabeled jira tickets\n",
    "jira_data_dir = \"./data/jira.csv\"\n",
    "jira_df = pd.read_csv(jira_data_dir, converters={'summary':convert_to_str,'description':convert_to_str})\n",
    "\n",
    "# Define jira data texts field\n",
    "jira_df[\"text\"] = jira_df[[\"summary\", \"description\"]].agg(' '.join, axis=1)\n",
    "\n",
    "# Pre-process text\n",
    "jira_df[\"text\"] = jira_df[\"text\"].apply(lambda s: preprocess_text(s))\n",
    "\n",
    "# Only keep tickets with non-empty text\n",
    "jira_df = jira_df[jira_df[\"text\"] != \"\"]\n",
    "\n",
    "# Choose final data fields\n",
    "jira_df = jira_df[[\"ticket_num\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create text file with all available text.\n",
    "To be used in pre-training for language model.\n",
    "\"\"\"\n",
    "\n",
    "# Define all text data available\n",
    "all_text = incidents_df[\"text\"].to_list() + jira_df[\"text\"].to_list()\n",
    "\n",
    "# Save data\n",
    "save_all_text_dir = \"./data/all_text.txt\"\n",
    "with open(save_all_text_dir, 'w') as f:\n",
    "    for text in all_text:\n",
    "        f.write(\"%s\\n\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jira_data_dir = \"./data/jira.csv\"\n",
    "jira_df = pd.read_csv(jira_data_dir, converters={'summary':convert_to_str,'description':convert_to_str})\n",
    "jira_df[\"ticket_num\"].to_csv(\"./data/jira_ticket_num.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked language modeling (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs additional MLM pre-training of a pre-trained\n",
    "DistilBERT model using text data saved in ./data/all_text.txt.\n",
    "\n",
    "Trained for 3 epochs.\n",
    "Used batch size equal to 8.\n",
    "Resulting model saved in ./data/tmp/meeseeks-mlm-nostop.\n",
    "These parameters can be adjusted.\n",
    "\n",
    "Training time takes around 2 hours on 8GB M60 GPU.\n",
    "\"\"\"\n",
    "\n",
    "!python run_mlm.py \\\n",
    "    --model_name_or_path distilbert-base-uncased \\\n",
    "    --train_file ./data/all_text.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir ./distilbert-meeseeks-mlm \\\n",
    "    --line_by_line \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size=8 \\\n",
    "    --per_device_eval_batch_size=8 \\\n",
    "    --num_train_epochs=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Fine-tuning with Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define labeled data\n",
    "\"\"\"\n",
    "dataset = incidents_df[incidents_df[\"label\"]!=\"\"]\n",
    "texts = dataset[\"text\"].to_list()\n",
    "labels = dataset[\"label\"].to_list()\n",
    "targets = dataset[\"target\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Text Classification (STC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper methods\n",
    "\"\"\"\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # calculate accuracy and f1 score using sklearn functions\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"micro\")\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1 score': f1,\n",
    "    }\n",
    "\n",
    "def get_prediction(text):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    \n",
    "    # executing argmax function to get the candidate label\n",
    "    return inv_label_map[probs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extend Pytorch Dataset class for classification purpose\n",
    "\"\"\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT with MLM pre-training + STC fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tune and evaluate DistilBERT model pre-trained on unlabeled data\n",
    "\"\"\"\n",
    "\n",
    "num_folds = 5    # number of folds for cross-validation\n",
    "max_length = 512 # max sequence length for each document/sentence sample\n",
    "batch_size = 8   # training and evaluation batch size\n",
    "epochs = 32      # number of epochs to fine-tune\n",
    "valid_size = 16  # validation data size, use for early stopping\n",
    "early_stopping_patience = 5 # early stopping epoch patience before terminating\n",
    "learning_rate = 1e-5 # learning rate\n",
    "pretrained_model_dir = \"./distilbert-meeseeks-mlm\" # MLM pre-trained model directory\n",
    "\n",
    "# store global results\n",
    "y_true = [\"\"]*len(texts)\n",
    "y_pred = [\"\"]*len(texts)\n",
    "\n",
    "# store list of fold-averaged metrics\n",
    "list_pre_fine_acc = []\n",
    "list_pre_fine_prec_avg, list_pre_fine_rec_avg, list_pre_fine_f1_avg = [], [], []\n",
    "list_pre_fine_prec, list_pre_fine_rec, list_pre_fine_f1 = [], [], []\n",
    "\n",
    "# train and evaluate using cross validation (using stratified k-fold, can use regular k-fold)\n",
    "kf = StratifiedKFold(n_splits=num_folds)\n",
    "for train_index, test_index in tqdm(kf.split(texts, targets), total=kf.get_n_splits(texts, targets)):\n",
    "    \n",
    "    # define train and test data\n",
    "    train_texts, test_texts = [texts[idx] for idx in train_index], [texts[idx] for idx in test_index]\n",
    "    train_targets, test_targets = [targets[idx] for idx in train_index], [targets[idx] for idx in test_index]\n",
    "    train_labels, test_labels = [labels[idx] for idx in train_index], [labels[idx] for idx in test_index]\n",
    "    \n",
    "    # define train and train-validation data\n",
    "    valid_size = valid_size\n",
    "    (train_texts, valid_texts, train_targets, valid_targets, train_labels, valid_labels) = train_test_split(train_texts, train_targets, train_labels, test_size=valid_size, random_state=1, stratify=train_targets)\n",
    "    \n",
    "    # load tokenizer and model\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_dir)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(pretrained_model_dir, num_labels=len(label_map))\n",
    "    model.to(\"cuda\")\n",
    "    model.train()\n",
    "\n",
    "    # tokenize train and validation data\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "    valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "    # convert our tokenized data into a torch Dataset\n",
    "    train_dataset = Dataset(train_encodings, train_targets)\n",
    "    valid_dataset = Dataset(valid_encodings, valid_targets)\n",
    "\n",
    "    # define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        report_to=None,\n",
    "        output_dir='./results',                  # output directory\n",
    "        overwrite_output_dir = True,             # overwrite output directory\n",
    "        do_train=True,                           # train\n",
    "        do_eval=True,                            # evaluate during training\n",
    "        num_train_epochs=epochs,                 # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "        warmup_steps=250,                        # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                       # strength of weight decay\n",
    "        learning_rate=learning_rate,             # set learning rate \n",
    "        logging_dir='./logs',                    # directory for storing logs\n",
    "        load_best_model_at_end=True,             # load the best model when finished training (default metric is loss)\n",
    "        metric_for_best_model='eval_loss',       # validation metric for model selection\n",
    "        save_strategy=\"epoch\",                   # save based on epochs\n",
    "        logging_strategy=\"epoch\",                # log based on epochs\n",
    "        evaluation_strategy=\"epoch\",             # evaluate base on epochs\n",
    "#         logging_steps=logging_steps,             # log & save weights each logging_steps\n",
    "\n",
    "    )\n",
    "\n",
    "    # define trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience = early_stopping_patience)] # early stopping callback\n",
    "    )\n",
    "\n",
    "    # train and evaluate after training is done\n",
    "    trainer.train()\n",
    "    print(trainer.evaluate())\n",
    "\n",
    "    # save the fine tuned model & tokenizer\n",
    "    model_path = \"./meeseeks-distilbert-base-uncased\"\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    # load saved model & tokenizer\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "    # make predictions on held out test data set\n",
    "    data_loader = torch.utils.data.DataLoader(test_texts, batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model(**inputs)\n",
    "        probs = outputs[0].softmax(1)\n",
    "        preds = [inv_label_map[idx] for idx in probs.argmax(axis=1).cpu().numpy()]\n",
    "        predictions += preds\n",
    "    \n",
    "    # store local predictions\n",
    "    fold_y_pred = []\n",
    "    fold_y_true = []\n",
    "    for i, idx in enumerate(test_index):\n",
    "        y_pred[idx] = predictions[i]\n",
    "        y_true[idx] = test_labels[i]\n",
    "        fold_y_pred.append(predictions[i])\n",
    "        fold_y_true.append(test_labels[i])\n",
    "        \n",
    "    # compute fold metrics\n",
    "    fold_acc = accuracy_score(fold_y_true, fold_y_pred)\n",
    "    fold_prec_avg, fold_rec_avg, fold_f1_avg, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average='macro')\n",
    "    fold_prec, fold_rec, fold_f1, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average=None, labels=labels_list)\n",
    "    \n",
    "    # store results on list of fold metrics\n",
    "    list_pre_fine_acc.append(fold_acc)\n",
    "    list_pre_fine_prec_avg.append(fold_prec_avg)\n",
    "    list_pre_fine_rec_avg.append(fold_rec_avg)\n",
    "    list_pre_fine_f1_avg.append(fold_f1_avg)\n",
    "    list_pre_fine_prec.append(fold_prec)\n",
    "    list_pre_fine_rec.append(fold_rec)\n",
    "    list_pre_fine_f1.append(fold_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print global metrics\n",
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average and stdev of metrics over folds\n",
    "pre_fine_acc_mean = np.mean(list_pre_fine_acc)\n",
    "pre_fine_acc_std = np.std(list_pre_fine_acc)\n",
    "pre_fine_prec_avg_mean = np.mean(list_pre_fine_prec_avg)\n",
    "pre_fine_prec_avg_std = np.std(list_pre_fine_prec_avg)\n",
    "pre_fine_rec_avg_mean = np.mean(list_pre_fine_rec_avg)\n",
    "pre_fine_rec_avg_std = np.std(list_pre_fine_rec_avg)\n",
    "pre_fine_f1_avg_mean = np.mean(list_pre_fine_f1_avg)\n",
    "pre_fine_f1_avg_std = np.std(list_pre_fine_f1_avg)\n",
    "\n",
    "pre_fine_prec_mean = np.mean(list_pre_fine_prec, axis=0)\n",
    "pre_fine_prec_std = np.std(list_pre_fine_prec, axis=0)\n",
    "pre_fine_rec_mean = np.mean(list_pre_fine_rec, axis=0)\n",
    "pre_fine_rec_std = np.std(list_pre_fine_rec, axis=0)\n",
    "pre_fine_f1_mean = np.mean(list_pre_fine_f1, axis=0)\n",
    "pre_fine_f1_std = np.std(list_pre_fine_f1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results in pickle file\n",
    "# done because it takes a while to compute results and notebook can crash\n",
    "with open('./data/pre_fine.pkl', 'wb') as f:\n",
    "    pickle.dump([pre_fine_acc, pre_fine_prec_avg, pre_fine_rec_avg, pre_fine_f1_avg, pre_fine_prec, pre_fine_rec, pre_fine_f1, \\\n",
    "                list_pre_fine_acc, list_pre_fine_prec_avg, list_pre_fine_rec_avg, list_pre_fine_f1_avg, list_pre_fine_prec, list_pre_fine_rec, list_pre_fine_f1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results saved in pickle file\n",
    "with open('./data/pre_fine.pkl', 'rb') as f:\n",
    "    pre_fine_acc, pre_fine_prec_avg, pre_fine_rec_avg, pre_fine_f1_avg, pre_fine_prec, pre_fine_rec, pre_fine_f1, \\\n",
    "                list_pre_fine_acc, list_pre_fine_prec_avg, list_pre_fine_rec_avg, list_pre_fine_f1_avg, list_pre_fine_prec, list_pre_fine_rec, list_pre_fine_f1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del trainer, model, tokenizer\n",
    "del train_dataset, valid_dataset\n",
    "del train_encodings, valid_encodings\n",
    "del data_loader\n",
    "del inputs, outputs, probs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT with STC fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tune and evaluate DistilBERT model pre-trained on unlabeled data\n",
    "\"\"\"\n",
    "\n",
    "num_folds = 5    # number of folds for cross-validation\n",
    "max_length = 512 # max sequence length for each document/sentence sample\n",
    "batch_size = 8   # training and evaluation batch size\n",
    "epochs = 32      # number of epochs to fine-tune\n",
    "valid_size = 16  # validation data size, use for early stopping\n",
    "early_stopping_patience = 5 # early stopping epoch patience before terminating\n",
    "learning_rate = 1e-5 # learning rate\n",
    "\n",
    "# store global results\n",
    "y_true = [\"\"]*len(texts)\n",
    "y_pred = [\"\"]*len(texts)\n",
    "\n",
    "# store list of fold-averaged metrics\n",
    "list_fine_acc = []\n",
    "list_fine_prec_avg, list_fine_rec_avg, list_fine_f1_avg = [], [], []\n",
    "list_fine_prec, list_fine_rec, list_fine_f1 = [], [], []\n",
    "\n",
    "# train and evaluate using cross validation (using stratified k-fold, can use regular k-fold)\n",
    "kf = StratifiedKFold(n_splits=num_folds)\n",
    "for train_index, test_index in tqdm(kf.split(texts, targets), total=kf.get_n_splits(texts, targets)):\n",
    "    \n",
    "    # define train and test data\n",
    "    train_texts, test_texts = [texts[idx] for idx in train_index], [texts[idx] for idx in test_index]\n",
    "    train_targets, test_targets = [targets[idx] for idx in train_index], [targets[idx] for idx in test_index]\n",
    "    train_labels, test_labels = [labels[idx] for idx in train_index], [labels[idx] for idx in test_index]\n",
    "    \n",
    "    # define train and train-validation data\n",
    "    valid_size = valid_size\n",
    "    (train_texts, valid_texts, train_targets, valid_targets, train_labels, valid_labels) = train_test_split(train_texts, train_targets, train_labels, test_size=valid_size, random_state=1, stratify=train_targets)\n",
    "    \n",
    "    # load tokenizer and model\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_map))\n",
    "    model.to(\"cuda\")\n",
    "    model.train()\n",
    "\n",
    "    # tokenize train and validation data\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "    valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "    # convert our tokenized data into a torch Dataset\n",
    "    train_dataset = Dataset(train_encodings, train_targets)\n",
    "    valid_dataset = Dataset(valid_encodings, valid_targets)\n",
    "\n",
    "    # define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        report_to=None,\n",
    "        output_dir='./results',                  # output directory\n",
    "        overwrite_output_dir = True,             # overwrite output directory\n",
    "        do_train=True,                           # train\n",
    "        do_eval=True,                            # evaluate during training\n",
    "        num_train_epochs=epochs,                 # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "        warmup_steps=250,                        # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                       # strength of weight decay\n",
    "        learning_rate=learning_rate,             # set learning rate \n",
    "        logging_dir='./logs',                    # directory for storing logs\n",
    "        load_best_model_at_end=True,             # load the best model when finished training (default metric is loss)\n",
    "        metric_for_best_model='eval_loss',       # validation metric for model selection\n",
    "        save_strategy=\"epoch\",                   # save based on epochs\n",
    "        logging_strategy=\"epoch\",                # log based on epochs\n",
    "        evaluation_strategy=\"epoch\",             # evaluate base on epochs\n",
    "#         logging_steps=logging_steps,             # log & save weights each logging_steps\n",
    "\n",
    "    )\n",
    "\n",
    "    # define trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience = early_stopping_patience)] # early stopping callback\n",
    "    )\n",
    "\n",
    "    # train and evaluate after training is done\n",
    "    trainer.train()\n",
    "    print(trainer.evaluate())\n",
    "\n",
    "    # save the fine tuned model & tokenizer\n",
    "    model_path = \"./meeseeks-distilbert-base-uncased-nomlm\"\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    # load saved model & tokenizer\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "    # make predictions on held out test data set\n",
    "    data_loader = torch.utils.data.DataLoader(test_texts, batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model(**inputs)\n",
    "        probs = outputs[0].softmax(1)\n",
    "        preds = [inv_label_map[idx] for idx in probs.argmax(axis=1).cpu().numpy()]\n",
    "        predictions += preds\n",
    "    \n",
    "    # store local predictions\n",
    "    fold_y_pred = []\n",
    "    fold_y_true = []\n",
    "    for i, idx in enumerate(test_index):\n",
    "        y_pred[idx] = predictions[i]\n",
    "        y_true[idx] = test_labels[i]\n",
    "        fold_y_pred.append(predictions[i])\n",
    "        fold_y_true.append(test_labels[i])\n",
    "        \n",
    "    # compute fold metrics\n",
    "    fold_acc = accuracy_score(fold_y_true, fold_y_pred)\n",
    "    fold_prec_avg, fold_rec_avg, fold_f1_avg, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average='macro')\n",
    "    fold_prec, fold_rec, fold_f1, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average=None, labels=labels_list)\n",
    "    \n",
    "    # store results on list of fold metrics\n",
    "    list_fine_acc.append(fold_acc)\n",
    "    list_fine_prec_avg.append(fold_prec_avg)\n",
    "    list_fine_rec_avg.append(fold_rec_avg)\n",
    "    list_fine_f1_avg.append(fold_f1_avg)\n",
    "    list_fine_prec.append(fold_prec)\n",
    "    list_fine_rec.append(fold_rec)\n",
    "    list_fine_f1.append(fold_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print global metrics\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average and stdev of metrics over folds\n",
    "fine_acc_mean = np.mean(list_fine_acc)\n",
    "fine_acc_std = np.std(list_fine_acc)\n",
    "fine_prec_avg_mean = np.mean(list_fine_prec_avg)\n",
    "fine_prec_avg_std = np.std(list_fine_prec_avg)\n",
    "fine_rec_avg_mean = np.mean(list_fine_rec_avg)\n",
    "fine_rec_avg_std = np.std(list_fine_rec_avg)\n",
    "fine_f1_avg_mean = np.mean(list_fine_f1_avg)\n",
    "fine_f1_avg_std = np.std(list_fine_f1_avg)\n",
    "\n",
    "fine_prec_mean = np.mean(list_fine_prec, axis=0)\n",
    "fine_prec_std = np.std(list_fine_prec, axis=0)\n",
    "fine_rec_mean = np.mean(list_fine_rec, axis=0)\n",
    "fine_rec_std = np.std(list_fine_rec, axis=0)\n",
    "fine_f1_mean = np.mean(list_fine_f1, axis=0)\n",
    "fine_f1_std = np.std(list_fine_f1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results in pickle file\n",
    "# done because it takes a while to compute results and notebook can crash\n",
    "with open('./data/fine.pkl', 'wb') as f:\n",
    "    pickle.dump([fine_acc, fine_prec_avg, fine_rec_avg, fine_f1_avg, fine_prec, fine_rec, fine_f1, \\\n",
    "                list_fine_acc, list_fine_prec_avg, list_fine_rec_avg, list_fine_f1_avg, list_fine_prec, list_fine_rec, list_fine_f1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # load results saved in pickle file\n",
    "    with open('./data/fine.pkl', 'rb') as f:\n",
    "        fine_acc, fine_prec_avg, fine_rec_avg, fine_f1_avg, fine_prec, fine_rec, fine_f1, \\\n",
    "                    list_fine_acc, list_fine_prec_avg, list_fine_rec_avg, list_fine_f1_avg, list_fine_prec, list_fine_rec, list_fine_f1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del trainer, model, tokenizer\n",
    "del train_dataset, valid_dataset\n",
    "del train_encodings, valid_encodings\n",
    "del data_loader\n",
    "del inputs, outputs, probs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train logistic classifier with TF-IDF features\n",
    "\"\"\"\n",
    "num_folds = 5    # number of folds for cross-validation\n",
    "\n",
    "# store global results\n",
    "y_true = [\"\"]*len(texts)\n",
    "y_pred = [\"\"]*len(texts)\n",
    "\n",
    "# store list of fold-averaged metrics\n",
    "list_tfidf_acc = []\n",
    "list_tfidf_prec_avg, list_tfidf_rec_avg, list_tfidf_f1_avg = [], [], []\n",
    "list_tfidf_prec, list_tfidf_rec, list_tfidf_f1 = [], [], []\n",
    "\n",
    "# train and evaluate using cross validation (using stratified k-fold, can use regular k-fold)\n",
    "kf = StratifiedKFold(n_splits=num_folds)\n",
    "for train_index, test_index in tqdm(kf.split(texts, targets), total=kf.get_n_splits(texts, targets)):\n",
    "    \n",
    "    # define train and test data\n",
    "    train_texts, test_texts = [texts[idx] for idx in train_index], [texts[idx] for idx in test_index]\n",
    "    train_targets, test_targets = [targets[idx] for idx in train_index], [targets[idx] for idx in test_index]\n",
    "    train_labels, test_labels = [labels[idx] for idx in train_index], [labels[idx] for idx in test_index]\n",
    "    \n",
    "    # compute TF-IDF features based on training data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_X = vectorizer.fit_transform(train_texts)\n",
    "    test_X = vectorizer.transform(test_texts)\n",
    "    \n",
    "    # fit logistic regression model and make predictions on test data\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(train_X, train_targets)\n",
    "    predictions = logreg.predict(test_X)\n",
    "        \n",
    "    # store local predictions\n",
    "    fold_y_pred = []\n",
    "    fold_y_true = []\n",
    "    for i, idx in enumerate(test_index):\n",
    "        y_pred[idx] = inv_label_map[predictions[i]]\n",
    "        y_true[idx] = test_labels[i]\n",
    "        fold_y_pred.append(inv_label_map[predictions[i]])\n",
    "        fold_y_true.append(test_labels[i])\n",
    "        \n",
    "    # compute fold metrics\n",
    "    fold_acc = accuracy_score(fold_y_true, fold_y_pred)\n",
    "    fold_prec_avg, fold_rec_avg, fold_f1_avg, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average='macro')\n",
    "    fold_prec, fold_rec, fold_f1, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average=None, labels=labels_list)\n",
    "    \n",
    "    # store results on list of fold metrics\n",
    "    list_tfidf_acc.append(fold_acc)\n",
    "    list_tfidf_prec_avg.append(fold_prec_avg)\n",
    "    list_tfidf_rec_avg.append(fold_rec_avg)\n",
    "    list_tfidf_f1_avg.append(fold_f1_avg)\n",
    "    list_tfidf_prec.append(fold_prec)\n",
    "    list_tfidf_rec.append(fold_rec)\n",
    "    list_tfidf_f1.append(fold_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print global metrics\n",
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average and stdev of metrics over folds\n",
    "tfidf_acc_mean = np.mean(list_tfidf_acc)\n",
    "tfidf_acc_std = np.std(list_tfidf_acc)\n",
    "tfidf_prec_avg_mean = np.mean(list_tfidf_prec_avg)\n",
    "tfidf_prec_avg_std = np.std(list_tfidf_prec_avg)\n",
    "tfidf_rec_avg_mean = np.mean(list_tfidf_rec_avg)\n",
    "tfidf_rec_avg_std = np.std(list_tfidf_rec_avg)\n",
    "tfidf_f1_avg_mean = np.mean(list_tfidf_f1_avg)\n",
    "tfidf_f1_avg_std = np.std(list_tfidf_f1_avg)\n",
    "\n",
    "tfidf_prec_mean = np.mean(list_tfidf_prec, axis=0)\n",
    "tfidf_prec_std = np.std(list_tfidf_prec, axis=0)\n",
    "tfidf_rec_mean = np.mean(list_tfidf_rec, axis=0)\n",
    "tfidf_rec_std = np.std(list_tfidf_rec, axis=0)\n",
    "tfidf_f1_mean = np.mean(list_tfidf_f1, axis=0)\n",
    "tfidf_f1_std = np.std(list_tfidf_f1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a logistic classifier with doc2vec embeddings\n",
    "\"\"\"\n",
    "num_folds = 5    # number of folds for cross-validation\n",
    "epochs = 32      # number of epochs to train doc2vec model\n",
    "\n",
    "# Method to tokenize text\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# store global results\n",
    "y_true = [\"\"]*len(texts)\n",
    "y_pred = [\"\"]*len(texts)\n",
    "\n",
    "# store list of fold-averaged metrics\n",
    "list_doc2vec_acc = []\n",
    "list_doc2vec_prec_avg, list_doc2vec_rec_avg, list_doc2vec_f1_avg = [], [], []\n",
    "list_doc2vec_prec, list_doc2vec_rec, list_doc2vec_f1 = [], [], []\n",
    "\n",
    "# train and evaluate using cross validation (using stratified k-fold, can use regular k-fold)\n",
    "kf = StratifiedKFold(n_splits=num_folds)\n",
    "for train_index, test_index in tqdm(kf.split(texts, targets), total=kf.get_n_splits(texts, targets)):\n",
    "    \n",
    "    # define train and test data\n",
    "    train_texts, test_texts = [texts[idx] for idx in train_index], [texts[idx] for idx in test_index]\n",
    "    train_targets, test_targets = [targets[idx] for idx in train_index], [targets[idx] for idx in test_index]\n",
    "    train_labels, test_labels = [labels[idx] for idx in train_index], [labels[idx] for idx in test_index]\n",
    "    \n",
    "    # create tagged documents dataframes\n",
    "    train_df = pd.DataFrame(list(zip(train_labels, train_targets, train_texts)), columns =['label', 'target', 'text'])\n",
    "    test_df = pd.DataFrame(list(zip(test_labels, test_targets, test_texts)), columns =['label', 'target', 'text'])\n",
    "    train_tagged = train_df.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r['label']]), axis=1)\n",
    "    test_tagged = test_df.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r['label']]), axis=1)\n",
    "\n",
    "    # train doc2vec vectorizer on training data\n",
    "    model_dbow = Doc2Vec(workers=multiprocessing.cpu_count())\n",
    "    model_dbow.build_vocab(train_tagged.values)\n",
    "    model_dbow.train(train_tagged.values, total_examples=len(train_tagged.values), epochs=epochs)\n",
    "    \n",
    "    # compute doc2vec features\n",
    "    y_train, X_train = zip(*[(doc.tags[0], model_dbow.infer_vector(doc.words, steps=20)) for doc in train_tagged.values])\n",
    "    y_test, X_test = zip(*[(doc.tags[0], model_dbow.infer_vector(doc.words, steps=20)) for doc in test_tagged.values])\n",
    "\n",
    "    # fit logistic regression model and make predictions on test data\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    predictions = logreg.predict(X_test)\n",
    "    \n",
    "    # store local predictions      \n",
    "    fold_y_pred = []\n",
    "    fold_y_true = []\n",
    "    for i, idx in enumerate(test_index):\n",
    "        y_pred[idx] = predictions[i]\n",
    "        y_true[idx] = test_labels[i]\n",
    "        fold_y_pred.append(predictions[i])\n",
    "        fold_y_true.append(test_labels[i])\n",
    "      \n",
    "    # compute fold metrics\n",
    "    fold_acc = accuracy_score(fold_y_true, fold_y_pred)\n",
    "    fold_prec_avg, fold_rec_avg, fold_f1_avg, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average='macro')\n",
    "    fold_prec, fold_rec, fold_f1, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average=None, labels=labels_list)\n",
    "    \n",
    "    # store results on list of fold metrics\n",
    "    list_doc2vec_acc.append(fold_acc)\n",
    "    list_doc2vec_prec_avg.append(fold_prec_avg)\n",
    "    list_doc2vec_rec_avg.append(fold_rec_avg)\n",
    "    list_doc2vec_f1_avg.append(fold_f1_avg)\n",
    "    list_doc2vec_prec.append(fold_prec)\n",
    "    list_doc2vec_rec.append(fold_rec)\n",
    "    list_doc2vec_f1.append(fold_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print global metrics\n",
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average and stdev of metrics over folds\n",
    "doc2vec_acc_mean = np.mean(list_doc2vec_acc)\n",
    "doc2vec_acc_std = np.std(list_doc2vec_acc)\n",
    "doc2vec_prec_avg_mean = np.mean(list_doc2vec_prec_avg)\n",
    "doc2vec_prec_avg_std = np.std(list_doc2vec_prec_avg)\n",
    "doc2vec_rec_avg_mean = np.mean(list_doc2vec_rec_avg)\n",
    "doc2vec_rec_avg_std = np.std(list_doc2vec_rec_avg)\n",
    "doc2vec_f1_avg_mean = np.mean(list_doc2vec_f1_avg)\n",
    "doc2vec_f1_avg_std = np.std(list_doc2vec_f1_avg)\n",
    "\n",
    "doc2vec_prec_mean = np.mean(list_doc2vec_prec, axis=0)\n",
    "doc2vec_prec_std = np.std(list_doc2vec_prec, axis=0)\n",
    "doc2vec_rec_mean = np.mean(list_doc2vec_rec, axis=0)\n",
    "doc2vec_rec_std = np.std(list_doc2vec_rec, axis=0)\n",
    "doc2vec_f1_mean = np.mean(list_doc2vec_f1, axis=0)\n",
    "doc2vec_f1_std = np.std(list_doc2vec_f1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run a dummy classifier\n",
    "\"\"\"\n",
    "\n",
    "# store global results\n",
    "y_true = [\"\"]*len(texts)\n",
    "y_pred = [\"\"]*len(texts)\n",
    "\n",
    "# store list of fold-averaged metrics\n",
    "list_dummy_acc = []\n",
    "list_dummy_prec_avg, list_dummy_rec_avg, list_dummy_f1_avg = [], [], []\n",
    "list_dummy_prec, list_dummy_rec, list_dummy_f1 = [], [], []\n",
    "\n",
    "# train and evaluate using cross validation (using stratified k-fold, can use regular k-fold)\n",
    "kf = StratifiedKFold(n_splits=num_folds)\n",
    "for train_index, test_index in tqdm(kf.split(texts, targets), total=kf.get_n_splits(texts, targets)):\n",
    "    \n",
    "    # define train and test data\n",
    "    train_texts, test_texts = [texts[idx] for idx in train_index], [texts[idx] for idx in test_index]\n",
    "    train_targets, test_targets = [targets[idx] for idx in train_index], [targets[idx] for idx in test_index]\n",
    "    train_labels, test_labels = [labels[idx] for idx in train_index], [labels[idx] for idx in test_index]\n",
    "    \n",
    "    # fit dummy classifier and make predictiosn on test data\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "    dummy_clf.fit([0]*len(train_texts), train_targets)\n",
    "    predictions = dummy_clf.predict([0]*len(test_texts))\n",
    "    \n",
    "    # store local predictions\n",
    "    fold_y_pred = []\n",
    "    fold_y_true = []\n",
    "    for i, idx in enumerate(test_index):\n",
    "        y_pred[idx] = inv_label_map[predictions[i]]\n",
    "        y_true[idx] = test_labels[i]\n",
    "        fold_y_pred.append(inv_label_map[predictions[i]])\n",
    "        fold_y_true.append(test_labels[i])\n",
    "        \n",
    "    # compute fold metrics\n",
    "    fold_acc = accuracy_score(fold_y_true, fold_y_pred)\n",
    "    fold_prec_avg, fold_rec_avg, fold_f1_avg, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average='macro')\n",
    "    fold_prec, fold_rec, fold_f1, _ = precision_recall_fscore_support(fold_y_true, fold_y_pred, average=None, labels=labels_list)\n",
    "    \n",
    "    # store results on list of fold metrics\n",
    "    list_dummy_acc.append(fold_acc)\n",
    "    list_dummy_prec_avg.append(fold_prec_avg)\n",
    "    list_dummy_rec_avg.append(fold_rec_avg)\n",
    "    list_dummy_f1_avg.append(fold_f1_avg)\n",
    "    list_dummy_prec.append(fold_prec)\n",
    "    list_dummy_rec.append(fold_rec)\n",
    "    list_dummy_f1.append(fold_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print global metrics\n",
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average and stdev of metrics over folds\n",
    "dummy_acc_mean = np.mean(list_dummy_acc)\n",
    "dummy_acc_std = np.std(list_dummy_acc)\n",
    "dummy_prec_avg_mean = np.mean(list_dummy_prec_avg)\n",
    "dummy_prec_avg_std = np.std(list_dummy_prec_avg)\n",
    "dummy_rec_avg_mean = np.mean(list_dummy_rec_avg)\n",
    "dummy_rec_avg_std = np.std(list_dummy_rec_avg)\n",
    "dummy_f1_avg_mean = np.mean(list_dummy_f1_avg)\n",
    "dummy_f1_avg_std = np.std(list_dummy_f1_avg)\n",
    "\n",
    "dummy_prec_mean = np.mean(list_dummy_prec, axis=0)\n",
    "dummy_prec_std = np.std(list_dummy_prec, axis=0)\n",
    "dummy_rec_mean = np.mean(list_dummy_rec, axis=0)\n",
    "dummy_rec_std = np.std(list_dummy_rec, axis=0)\n",
    "dummy_f1_mean = np.mean(list_dummy_f1, axis=0)\n",
    "dummy_f1_std = np.std(list_dummy_f1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of metric means\n",
    "df = pd.DataFrame({\n",
    "    'Score': ['Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "    'Dummy stratified': [dummy_acc_mean, dummy_prec_avg_mean, dummy_rec_avg_mean, dummy_f1_avg_mean],\n",
    "    'Doc2vec': [doc2vec_acc_mean, doc2vec_prec_avg_mean, doc2vec_rec_avg_mean, doc2vec_f1_avg_mean],\n",
    "    'TF-IDF': [tfidf_acc_mean, tfidf_prec_avg_mean, tfidf_rec_avg_mean, tfidf_f1_avg_mean],\n",
    "    'DistilBERT STC ': [fine_acc_mean, fine_prec_avg_mean, fine_rec_avg_mean, fine_f1_avg_mean],\n",
    "    'DistilBERT MLM+STC': [pre_fine_acc_mean, pre_fine_prec_avg_mean, pre_fine_rec_avg_mean, pre_fine_f1_avg_mean],\n",
    "})\n",
    "\n",
    "# create dataframe of metric stdevs\n",
    "df_std = pd.DataFrame({\n",
    "    'Score': ['Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "    'Dummy stratified': [dummy_acc_std, dummy_prec_avg_std, dummy_rec_avg_std, dummy_f1_avg_std],\n",
    "    'Doc2vec': [doc2vec_acc_std, doc2vec_prec_avg_std, doc2vec_rec_avg_std, doc2vec_f1_avg_std],\n",
    "    'TF-IDF': [tfidf_acc_std, tfidf_prec_avg_std, tfidf_rec_avg_std, tfidf_f1_avg_std],\n",
    "    'DistilBERT STC ': [fine_acc_std, fine_prec_avg_std, fine_rec_avg_std, fine_f1_avg_std],\n",
    "    'DistilBERT MLM+STC': [pre_fine_acc_std, pre_fine_prec_avg_std, pre_fine_rec_avg_std, pre_fine_f1_avg_std],\n",
    "})\n",
    "\n",
    "# reformat dataframes\n",
    "tidy = df.melt(id_vars='Score', var_name='Model').rename(columns=str.title)\n",
    "tidy_std = df_std.melt(id_vars='Score', var_name='Model').rename(columns=str.title)\n",
    "\n",
    "# create plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.barplot(x='Score', y='Value', hue='Model', data=tidy, ax=ax)\n",
    "\n",
    "# add error bars\n",
    "x_coords = [p.get_x() + 0.5*p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_height() for p in ax.patches]\n",
    "plt.errorbar(x=x_coords, y=y_coords, yerr=tidy_std[\"Value\"], fmt=\"none\", c= \"k\", capsize=7, elinewidth=1.4)\n",
    "\n",
    "# set plot limits\n",
    "ax.set(ylim=(0,1))\n",
    "plt.yticks(np.arange(0, 1.1, .1))\n",
    "# plt.title('Scores')\n",
    "sns.despine(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of metric means\n",
    "df = pd.DataFrame({\n",
    "    'Label': labels_list,\n",
    "    'Dummy stratified': dummy_f1_mean,\n",
    "    'Doc2vec': doc2vec_f1_mean,\n",
    "    'TF-IDF': tfidf_f1_mean,\n",
    "    'DistilBERT STC': fine_f1_mean,\n",
    "    'DistilBERT MLM+STC': pre_fine_f1_mean \n",
    "})\n",
    "\n",
    "# create dataframe of metric stdevs\n",
    "df_std = pd.DataFrame({\n",
    "    'Label': labels_list,\n",
    "    'Dummy stratified': dummy_f1_std,\n",
    "    'Doc2vec': doc2vec_f1_std,\n",
    "    'TF-IDF': tfidf_f1_std,\n",
    "    'DistilBERT STC': fine_f1_std,\n",
    "    'DistilBERT MLM+STC': pre_fine_f1_std \n",
    "})\n",
    "\n",
    "# reformat dataframes\n",
    "tidy = df.melt(id_vars='Label', value_name='F1', var_name='Model').rename(columns=str.title)\n",
    "tidy_std = df_std.melt(id_vars='Label', value_name='F1', var_name='Model').rename(columns=str.title)\n",
    "\n",
    "# order data labels according to data size\n",
    "# Note: This is ad-hoc. Change according to chosen labels and their size.\n",
    "ord_map = {\"Deploy\":0, \"Other\":1, \"Downstream\":2, \"Noise\":3, \"Outlier Bad Hosts\":4}\n",
    "model_map = {\"Dummy stratified\":0, \"Doc2vec\":1, \"TF-IDF\":2, \"DistilBERT STC\":3, \"DistilBERT MLM+STC\":4}\n",
    "tidy[\"target\"] = tidy[\"Label\"].apply(lambda x: ord_map[x])\n",
    "tidy[\"model_id\"] = tidy[\"Model\"].apply(lambda x: model_map[x])\n",
    "tidy = tidy.sort_values(by=[\"model_id\", \"target\"])\n",
    "tidy_std[\"target\"] = tidy_std[\"Label\"].apply(lambda x: ord_map[x])\n",
    "tidy_std[\"model_id\"] = tidy_std[\"Model\"].apply(lambda x: model_map[x])\n",
    "tidy_std = tidy_std.sort_values(by=[\"model_id\", \"target\"])\n",
    "\n",
    "# create plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "sns.barplot(x='Label', y='F1', hue='Model', data=tidy, ax=ax)\n",
    "\n",
    "# add error bars\n",
    "x_coords = [p.get_x() + 0.5*p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_height() for p in ax.patches]\n",
    "plt.errorbar(x=x_coords, y=y_coords, yerr=tidy_std[\"F1\"], fmt=\"none\", c= \"k\", capsize=7, elinewidth=1.4)\n",
    "\n",
    "# set plot limits\n",
    "ax.set(ylim=(0,1))\n",
    "plt.yticks(np.arange(0, 1.1, .1))\n",
    "# plt.title(\"F1 Score across labels\")\n",
    "sns.despine(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from bertopic import BERTopic\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data\n",
    "docs = incidents_df[\"text\"].to_list()\n",
    "labels = incidents_df[\"label\"].to_list()\n",
    "targets = incidents_df[\"target\"].to_list()\n",
    "classes = [label if label in label_map else \"Unlabeled\" for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With DistilBERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(labels_list)\n",
    "\n",
    "# embedding model\n",
    "model = TransformerDocumentEmbeddings(\"distilbert-base-uncased\", layers=\"-1\")\n",
    "\n",
    "# fit and reduce topic model\n",
    "topic_model = BERTopic(verbose=True, embedding_model=model)\n",
    "topias, probs = topic_model.fit_transform(docs)\n",
    "topics, probs = topic_model.reduce_topics(docs, topics, probs, nr_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = []\n",
    "for i in tqdm(range(len(docs))):\n",
    "    sent = Sentence(docs[i])\n",
    "    model.embed(sent)\n",
    "    embeddings.append(sent.embedding.detach().cpu().numpy())\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With DistilBERT finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('./meeseeks-distilbert-base-uncased/')\n",
    "model = DistilBertModel.from_pretrained('./meeseeks-distilbert-base-uncased').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to aggregate token embeddings across document\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "max_length = 512\n",
    "embeddings = []\n",
    "for i in tqdm(range(len(docs))):\n",
    "    doc = docs[i]\n",
    "    inputs = tokenizer(doc, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outs = model(**inputs)\n",
    "    embedding = mean_pooling(outs, inputs[\"attention_mask\"])\n",
    "    embedding = embedding.squeeze().detach().cpu().numpy()\n",
    "    torch.cuda.synchronize()\n",
    "    embeddings.append(embedding)\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and reduce topic model\n",
    "num_topics = len(labels_list)\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "topics, probs = topic_model.reduce_topics(docs, topics, probs, nr_topics=num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(i, topic_model.get_representative_docs(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(i, [word for (word, prob) in topic_model.get_topic(i)[:10]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [i for i, cls in enumerate(classes) if cls!=\"Unlabeled\"]\n",
    "ndocs = [docs[i] for i in mask]\n",
    "ntopics = [topics[i] for i in mask]\n",
    "nclasses = [classes[i] for i in mask]\n",
    "topics_per_class = topic_model.topics_per_class(ndocs, ntopics, classes=nclasses)\n",
    "fig_semi_supervised = topic_model.visualize_topics_per_class(topics_per_class)\n",
    "fig_semi_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize topics in UMAP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"labels\"] = topics\n",
    "result[\"targets\"] = labels\n",
    "mapping = {'':'.',\n",
    "           'Downstream':'v',\n",
    "           'Noise':'x',\n",
    "           'Deploy':'s',\n",
    "           'Outlier Bad Hosts':'p',\n",
    "           'Other':'+',\n",
    "           'Garbage Collection':'v',\n",
    "           'Capacity':'+',\n",
    "           'Database':'s',\n",
    "           'Upstream':'^',\n",
    "           'CM':'p',\n",
    "           'Infrastructure':'H'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = []\n",
    "sns.reset_orig()\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "groups = clustered.groupby(\"targets\")\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=5)\n",
    "legend_labels.append(\"Outliers\")\n",
    "cmap = plt.get_cmap('rainbow', num_topics)\n",
    "for (name, group) in groups:\n",
    "    if name == \"\":\n",
    "        plt.scatter(group.x, group.y, c=group.labels, s=15, cmap=cmap, marker=\".\")\n",
    "        legend_labels.append(\"Unlabeled\")\n",
    "    else:\n",
    "        plt.scatter(group.x, group.y, c=group.labels, s=130, cmap=cmap, marker=mapping[name])\n",
    "        legend_labels.append(name)\n",
    "ax.legend(labels=legend_labels)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0, cmap=cmap)\n",
    "plt.xlim([-1,17])\n",
    "plt.ylim([-3,18])\n",
    "# fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: x)\n",
    "plt.colorbar(ticks=np.arange(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
